---
sidebar: aiSidebar
title: AI 共有的不足和缺陷
---

# AI 共有的不足和缺陷

## 数据与知识层面

- 训练数据不可追溯：来源不透明，更新不可再现，难以审计和监管（例：医疗对话数据混入论坛内容，错误建议无法追责）。
- 时效性缺失：回答容易引用过时信息，对实时数据与趋势不敏感（例：被问到最新利率或 CVE 时仍返回去年数据）。
- 事实幻觉：缺少可验证引用，虚构数字、文献和代码依然常见（例：编造论文标题、捏造 npm 包版本或接口字段）。
- 领域不均衡：中文、少数语言、专业术语、方言与长尾格式支持薄弱（例：中医方剂、法律条文引用常错位，粤语/方言指令被误解）。
- 偏见与刻板印象：历史数据里的歧视被放大，跨地区/行业使用易冲突本地法规与文化（例：求职推荐偏向性别刻板印象，触犯当地公平招聘规定）。
- 检索依赖：RAG 质量受索引时效、噪声和稀疏影响，无法一劳永逸解决事实性（例：索引未刷新导致产品文档已改版但回答仍指向旧参数）。

## 模型与算法局限

- 概率当确定性：缺少置信度表达，难以分级处理高风险场景（例：医疗问答把低概率诊断当肯定结论）。
- 长文本脆弱：上下文窗口有限，早期约束易被遗忘，推理链断裂（例：合同审阅漏掉前文免除条款）。
- 多模态裁决不足：图文冲突时缺少稳健的决策逻辑，新攻击面频出（例：图中文字写“禁止吸烟”但模型按图像场景回答“可以吸烟”）。
- 逻辑/数值/物理推理弱：多步因果、数量守恒、空间推理常犯低级错误（例：流水线产能计算时把单位搞混、数量不守恒）。
- 微调与漂移：定向微调易被脏数据污染，旧能力遗忘，新行为难预测（例：加入少量脏数据后把安全回复改成敏感输出）。
- 解释性薄弱：难给出因果链与可追溯证据，审核与合规成本高（例：风控拒绝原因无法溯源到具体特征或规则）。

## 系统与工程稳定性

- 输出波动与延迟：服务抖动、偶发崩溃，难以满足 SLA 与韧性要求（例：高峰期响应从 1s 抖到 20s+）。
- 格式不稳定：结构化输出常偏离协议，需要大量后处理与人工兜底（例：约定 JSON 却混入自然语言或缺字段）。
- 工具/函数调用脆弱：异常返回、超时、限流易让模型陷入死循环或空洞回复（例：重试未设上限导致 API 雪崩）。
- 成本不可控：冗长上下文、重试和插件调用叠加，账单常超预期（例：一次客服对话因多次检索与重试耗费数十倍预估成本）。
- 资源与供应链风险：GPU/TPU 供应紧张或权重合规受限会整体拖垮服务质量（例：突发算力抢占导致延迟飙升）。
- 监控与回溯不足：缺少在线评测、自动报警和可重放数据，问题多依赖用户投诉暴露（例：生产错误格式连续数小时未被发现）。
- 代码执行高风险：AI 生成或运行的脚本若无隔离，可能修改全局配置甚至误删磁盘（例：未在沙箱跑安装脚本导致 `rm -rf /` 误删服务器数据）。

## 安全、对抗与隐私

- 提示注入与越权：系统提示泄露、对抗样本和花式绕过依然高效（例：用户通过“忽略之前指令”拿到后台工具调用说明）。
- 隐私泄漏：模型记忆输入，日志和回答中可能暴露敏感信息（例：复述先前工单中的手机号或订单号）。
- 价值观与伦理红线：安全策略在多地区/行业下易失效，输出仍可能触碰红线（例：未屏蔽地区敏感话题或行业合规禁区）。
- 权限与会话隔离弱：多用户上下文混用，存在跨会话泄漏风险（例：多人协作时引用了其他用户的历史聊天内容）。
- 安全基线缺失：缺少灾难恢复、降级与熔断策略，异常时容易雪崩（例：上游检索宕机导致整体对话不可用，没有降级答案）。

## 产品化与运营挑战

- 需求澄清能力弱：遇到模糊请求时少主动追问，倾向冗长且模糊的回答（例：用户问“做个活动页”未反问目标、预算、设备渠道）。
- 用户体验漂移：同一问题答案随时间、状态或微小措辞变化大，难以做稳定运营（例：FAQ 一天内多次输出不同价格策略）。
- 人力成本高：提示词工程、对话状态管理和后处理需要持续人工投入（例：为保持 JSON 输出要不断调整 system prompt）。
- 反馈闭环慢：负面反馈难以被快速吸收，模型迭代周期长，线上修复滞后（例：用户举报错误后需多周才能进入新版本权重）。
- 规则对齐难：模型不了解业务 KPI、调用成本或限流策略，易触发额外成本或风险（例：忽略调用限额反复触发付费 API 导致账单爆表）。

## 典型高风险场景

- 医疗、金融、法律：事实幻觉、过时信息与偏见会直接影响合规和人身/财产安全（例：给出不符合当地指南的用药建议；引用废止条款）。
- 工业控制、自动驾驶、能源调度：延迟、抖动和边界条件下的不稳定推理难以满足安全要求（例：异常传感器数据时仍输出正常指令）。
- 内容审核、舆情与推荐：偏见、对抗样本与格式不稳定会放大审核漏报或误报（例：被对抗样本绕过暴恐文本检测）。
- 代码与配置生成：忽略运行时依赖、版本兼容与安全基线，可能引入漏洞（例：生成的依赖存在已知 CVE，或配置未加鉴权）。
- 客服与多轮对话：错误累积、缺乏自我纠错和澄清，容易形成错误链（例：一次误解订单号后续全程使用了错误上下文）。

## 人的主导作用与落地原则

- 人定流程，机做辅助：关键环节先定义人类决策流程，AI 仅做草稿、对照或检索；最终决策与签字由人负责（例：招投标由人定评分表，AI 只初筛标书）。
- 明确“人类监督”角色：指定责任人审核高风险输出（医疗/金融/法律/自动驾驶/安全变更），建立双人复核或结对审查（例：医疗报告需主治与质控双人签字，AI 草稿仅供参考）。
- 人保准入，机做建议：让 AI 给选项而非直接执行，审批、下单、推送、发布、调度等动作必须经过人工确认（例：AI 生成折扣方案，运营手工确认后再上线）。
- 人控知识，机用引用：知识库由人维护版本与生效时间，AI 只能引用，不得自行增删；更新流程需人工验收与回溯标签（例：客服 FAQ 由知识管理员审核后上线，并标注生效日期）。
- 人验安全，机跑流程：安全策略、脱敏规则、合规模板由人制定；AI 输出必须过人设的校验（格式、术语、敏感词、地区合规）（例：代码生成需通过人写的安全扫描规则后才能提交）。
- 人管成本，机限配额：费用预算、配额和重试上限由人配置，AI 调用受限，超限直接降级或中止（例：每日检索调用超过配额时自动转为摘要模式并通知负责人）。
- 人做澄清，机做收敛：对模糊需求，先由人或前置问卷澄清核心参数，再让 AI 生成；避免 AI 自行假设（例：活动页需求先收集预算和渠道，再让 AI 出页面草稿）。
- 人设指标，机供度量：业务 KPI、安全阈值、可解释性要求由人定义；AI 需要输出置信度/引用/可追溯数据供人审阅（例：风控模型需附置信度和引用规则，审核员决定是否拒绝）。
- 人定迭代节奏：线上反馈、红队结果与误报/漏报统计由人评审，决定是否更新提示词、检索索引或模型版本（例：每周复盘高风险对话，人工决定是否切换新模型）。

## 应对建议（简要）

- 数据：做好来源审计、去偏与时效刷新，检索链路监控召回/精排质量（例：每周重建索引并人工抽检医疗条目）。
- 模型：提供置信度或自评估，限制长上下文依赖，针对关键任务做专项评测（例：财报问答需展示置信度并通过专门算例测试）。
- 系统：加熔断、降级和重试上限，强制结构化输出校验，建立在线监控与回溯（例：工具调用超时即降级为静态答案并记录重放日志）。
- 安全：定期红队，对抗样本库更新，隔离系统提示与用户输入，保护日志与隐私（例：每月红队拉通提示注入用例，更新过滤策略）。
- 运营：标准化提示与模板，AB/灰度发布，快速回滚通道，建立反馈闭环与标签体系（例：新客服提示先灰度 5% 流量，异常立即回滚）。

把模型当作不稳定的外部依赖来治理：先明确人类的职责、审批和复核流程，再让 AI 进入关键路径；留足监控、评测、韧性与人工兜底的预算，确保“人是决策者，AI 是工具”。
